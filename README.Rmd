---
title: "Project 2"
author: "Yilin Xie"
date: "July 3, 2020"
params: 
      weekday: 
       label: "weekday:"
       value: monday
       input: select
       choices: [monday, tuesday, wednesday, thursday, friday, saturday, sunday]
output: 
  rmarkdown::github_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The perpose of this project is going to analyze an online news popularity data set [here](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) and predict **shares** by backward linear regression and random forest. Firstly I read it into R session and determine which variables that I would deal with.

```{r}
#Load the data
news <- read.csv("/Users/yilinxie/Desktop/ST558/Project/Project2/OnlineNewsPopularity.csv")
head(news)
#detect NA value
sum(is.na(news$shares) )
```

As shown above, it is a huge data set with 39644 rows and 61 colomns. It contains factor variables, numeric variables and dummy variables. The task is to predict the 61st variable **shares**. Then I'm going to preprocess the data to get it in the form I need.

### Data preprocessing

```{r warning=FALSE,message=FALSE}
library(tidyverse)
## Filters out the data for the specified weekday
news <- mutate(news, weekday = ifelse(weekday_is_monday == "1", "monday", ifelse(weekday_is_tuesday == "1", "tuesday", ifelse(weekday_is_wednesday == "1", "wednesday", ifelse(weekday_is_thursday == "1", "thursday", ifelse(weekday_is_friday, "friday", ifelse(weekday_is_saturday == "1", "saturday", "sunday")))))))
news1 <- filter(news, weekday == params$weekday)
## Remove the useless colomns
news1 <- select(news1, -url, -weekday_is_monday, -weekday_is_tuesday, -weekday_is_wednesday, -weekday_is_thursday, -weekday_is_friday, -weekday_is_saturday, -weekday_is_sunday, -is_weekend, -weekday)
r = nrow(news1)
c = ncol(news1)
r
c
```

Since r>>c, this is not a high dimension data set. I would use and to predict **shares** by the entire variables.

### Data split

```{r}
#Set seed to make work reproducible
set.seed(123)
#randomly sample from the data
sub <- sample(1:r, 0.7*r)
#store training dataset (70% of the data) and test dataset (30% of the data)
train <- news1[sub,]
test <- news1[-sub,]
```

### Summarizations

We can look at the distribution of **shares** through the histogram and see some statistics of the total variables in a summary table.

```{r}
#number of rows of training dataset
nrow(train)
#draw histograms
hist(train$shares)
hist(log(train$shares))
#summary the training dataset
t(summary(train))
```

## Ensemble model fit

A random forest is a forest constructed in a random way, and the forest is composed of many unrelated decision trees. Therefore, in theory, the performance of random forest is generally better than that of a single decision tree, because the results of random forest are determined by voting on the results of multiple decision trees. But here, I'm using a random forest for regression.

### On train set

```{r fig.height=6, fig.width=10, message=FALSE}
#load package
library(randomForest)
#Get random forest model fit on training dataset
rf <- randomForest(shares ~ ., data = train, importance=TRUE)
rf
#variable importance measures
importance(rf)
#draw dotplot of variable importance as measured by Random Forest
varImpPlot(rf)
```


Calculate the predicted mean square error on the train set:

```{r}
train.pred <- predict(rf, train[,-52])
mean((train.pred - train$shares)^2)
```

So, the predicted mean square error on the training dataset is 68724773.  

### On test set
```{r}
rf.test <- predict(rf, newdata = test[,-52])
mean((test$shares-rf.test)^2)
```

So, the predicted mean square error on the testing dataset is 80333539.  

## Linear regression fit

I choose stepwise regression to fit this model, more specificly, the backward way. This data set contains too many variables. And since I'm not an expert on journalism, I can't tell which factors should have a real effect on the predicted variables. If I manually removed the variables by their significance in the model and compared the differences between the models, this would be a lot of work. So I want to use a backward regression model based on the AIC criteria to automatically determine which variables should be included or removed from the model.

### On train set

```{r}
#fit model
lm.step <- step(lm(shares ~ .,data = train))
```
```{r}
#summary the model
summary(lm.step)
```

Calculate the predicted mean square error on the train set:

```{r}
train.pred <- predict(lm.step, train[,-52])
mean((train.pred - train$shares)^2)
```

So, the predicted mean square error on the training dataset is 264157333.  

### On test set

Calculate the predicted mean square error on the test set:

```{r}
test.pred <- predict(lm.step, test[,-52])
mean((test.pred - test$shares)^2)
```

So, the predicted mean square error on the testing dataset is 85833271.  

## Conclusions

In this project, random forest and backward stepwise regression were used to predict the number of shares in social networks (popularity) about articles published by Mashable in a period of two years. Random forest is a kind of ensemble learning, which uses decision tree as weak classifier. Backward stepwise regression gradually removes the less significant variables on the basis that all variables are included in the regression model. As for data collected about Monday, shares of referenced articles in Mashable, average keyword and average shares of worst keyword are variables considered important to the results by both models. The random forest showed slight overfitting and backward stepwise regression showed severe underfitting. However, the two eventually performed similarly on the test set with random forest slightly superior to backward stepwise regression. But this is not the strictest way to compare the performance of the two models. Multiple data sets can be divided and predicted to calculate the mean MSE.



